{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "<style>\n",
    "    @media print{\n",
    "        body {\n",
    "            position:relative !important;\n",
    "        }\n",
    "        .celltag_new_page {\n",
    "            page-break-before: always !important;\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "# COMPSCI 371 Homework 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partners: Brian Janger, Matthew Wang, Caleb Watson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AT"
    ]
   },
   "source": [
    "### Problem 0 (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 1: Kernels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.1 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step, we put the conditions in matrix form: \n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "K(x_1,x_1) & K(x_1,x_2)\\\\\n",
    "K(x_2,x_1) & K(x_2,x_2)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 2\\\\\n",
    "2 & 1\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that it is a valid Kernel, we test if the matrix is positive semi-definite. We find the eigenvalues of the symmetrical matrix. \n",
    "\n",
    "The eigenvalues of the matrix are $\\lambda_1=-1, \\lambda_2=3$. Since one of the eigenvalues is negative, the matrix is not positive semi-definite. Therefore, no function $\\rho(x)$ exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.2 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $K(x,\\xi)=(x^T\\xi+c)^2$ in $R^3$ is $K(x,\\xi)=(x^T\\xi+c)^2=(x_1\\xi_1 + x_2\\xi_2 + x_3\\xi_3 + c)^2$\n",
    "\n",
    "Expanding the function out, we get \n",
    "\n",
    "$x_1^2\\xi_1^2 + x_2^2\\xi_2^2 + x_3^2\\xi_3^2 + 2x_1x_2\\xi_1\\xi_2 + 2x_1x_3\\xi_1\\xi_3 + 2x_2x_3\\xi_2\\xi_3 + 2x_1\\xi_1c + 2x_2\\xi_2c + 2x_3\\xi_3c + c^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore \n",
    "\n",
    "$\\rho(x)=\\rho(x_1,x_2,x_3)=(c,x_1,x_2,x_3,\\sqrt{2}x_1x_2,\\sqrt{2}x_1x_3,\\sqrt{2}x_2x_3,\\sqrt{2}x_1,\\sqrt{2}x_2,\\sqrt{2}x_3)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 monomials, so $e=10$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.3 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$e={d+k \\choose k}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n, d = 20, 30\n",
    "x = np.random.randn(n, d)\n",
    "\n",
    "def check_rbf_kernel(x, sigma=1.):\n",
    "    \n",
    "    \n",
    "check_rbd_kernel(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 2: The Representer Theorem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os import path as osp\n",
    "\n",
    "def retrieve(file_name, semester='fall22', course='371', homework=6):\n",
    "    if osp.exists(file_name):\n",
    "        print('Using previously downloaded file {}'.format(file_name))\n",
    "    else:\n",
    "        fmt = 'https://www2.cs.duke.edu/courses/{}/compsci{}/homework/{}/{}'\n",
    "        url = fmt.format(semester, course, homework, file_name)\n",
    "        urlretrieve(url, file_name)\n",
    "        print('Downloaded file {}'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false,
    "tags": [
     "AST"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using previously downloaded file ad.pickle\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "pickle data was truncated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m retrieve(ad_data_file)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(ad_data_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 6\u001b[0m     ad_data \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: pickle data was truncated"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "ad_data_file = 'ad.pickle'\n",
    "retrieve(ad_data_file)\n",
    "with open(ad_data_file, 'rb') as file:\n",
    "    ad_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.1 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The representer theorem does hold for $L_{\\text{reg}}(v) = \\left\\lVert v \\right\\rVert^2 + CL_T(v)$ because it matches the general formulation of the representer theorem (assuming $v$ is a training observation where $v \\in \\mathbb{R}^N$). The representer theorem requires a strictly increasing function $R$ from $\\mathbb{R}_+$ to $\\mathbb{R}$ and any function $S$ from $\\mathbb{R}^N$ to $\\mathbb{R}$.\n",
    "\n",
    "We see that for the function $R(a) = a^2$ and the function $S(v) = CL_T(v)$, the LRC risk function can be rewritten as $L_{\\text{reg}}(v) = R(\\left\\lVert v \\right\\rVert) + S(v)$, which matches the general formulation of the representer theorem. Therefore, the reprenter theorem holds for LRCs trained with regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.2 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The representer theorem does not hold for $L_T(v) = -\\frac{1}{N}\\sum\\limits_{n=1}^N[y_n\\log p_n + (1-y_n)\\log(1-p_n)]$ where $p_n = \\frac{1}{e^{-w^Tx_n-b}}$ because we violate the proof assumption that we have a strictly increasing function $R$ from $\\mathbb{R}_+$ to $\\mathbb{R}$ in our training risk function. All we have in the standard cross-entropy loss function is a single term, which serves as the function $S$ from $\\mathbb{R}^N$ to $R$.\n",
    "\n",
    "The proof is violated specifically at inequality (6) in the class notes, where we use the function $R$ to justify the implication that $R(\\left\\lVert w \\right\\rVert) < R(\\left\\lVert w* \\right\\rVert)$ (where $w = w^* - u, u \\neq 0$). \n",
    "\n",
    "Since the proof also shows that our function $S$ has the property $S(w^Tx_1+b,...,w^Tx_N+b) = S((w^*)^Tx_1+b,...,(w^*)^Tx_N+b)$, the proof relies on the fact that inequality (6) exists to prove that $L(w,b) < L(w^*,b)$. Since this statement isn't valid anymore, we can't say that $w^*$ is the optimal vector, and therefore the vector $u \\in X^\\perp$ can be nonzero, meaning the hyerplane cannot be expressed as a linear combination of the input vectors $x_1,...,x_N \\in X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false,
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate(h, data, h_name):\n",
    "    def accuracy(s):\n",
    "        sx, sy = s['x'], s['y']\n",
    "        return h.score(sx, sy) * 100\n",
    "\n",
    "    train, test = data['train'], data['test']\n",
    "    f = '{:s}:\\n\\ttraining accuracy is {:.2f} percent,' +\\\n",
    "        '\\n\\ttest accuracy is {:.2f} percent'\n",
    "    print(f.format(h_name, accuracy(train), accuracy(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ad_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe residual for the optimal vector found by the training algorithm is \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe residual for a random vector is \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m experiment(LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m), \u001b[43mad_data\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLRC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ad_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def experiment(h, data, h_name): \n",
    "    # cross-validates classifier and fits to determine a regularization constant\n",
    "    cs = np.logspace(-3,2,15)\n",
    "    h = GridSearchCV(h, {'C': cs})\n",
    "    h = h.fit(data['train']['x'], data['train']['y'])\n",
    "    \n",
    "    # obtain training and testing accuracy\n",
    "    evaluate(h, data, h_name)\n",
    "\n",
    "    #printing remaining required information\n",
    "    print('\\tThe best value for the regularization constant C is %.'.format(h.C))\n",
    "    print('\\tThere are % training samples of dimensionality %.'.format(len(data['train']['x']), len(data['train']['x'][0])))\n",
    "    print('\\tThe number of linearly independent training samples is %.'.format(np.linalg.rank(data['train']['x'])))\n",
    "    \n",
    "    # calculating residuals for optimal and random vectors (only applicable to linear classifiers)\n",
    "    if hasattr(h, 'coef_'):\n",
    "        w = h.coef_.flatten()\n",
    "        random_w = np.random.rand(len(w))\n",
    "        \n",
    "        print('\\tThe residual for the optimal vector found by the training algorithm is %.'.format(residual(w, data['train']['x']))\n",
    "        print('\\tThe residual for a random vector is %.'.format(residual(random_w, data['train']['x']))\n",
    "        return h, w\n",
    "    else:\n",
    "        return h\n",
    "    \n",
    "# helper function to calculate residual\n",
    "def residual(vector, data):\n",
    "    # computing coefficients from the data points\n",
    "    beta = np.lingalg.lstsq(data)\n",
    "    \n",
    "    return np.lingalg.norm(vector - sum(beta*data))\n",
    "    \n",
    "experiment(LogisticRegression(max_iter=1000), ad_data, 'LRC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm, w_svm = experiment(SVC(kernel='linear'), ad_data, 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 3: Linear and Nonlinear SVMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false,
    "tags": [
     "AST"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using previously downloaded file data.pickle\n"
     ]
    }
   ],
   "source": [
    "data_2d_file_name = 'data.pickle'\n",
    "retrieve(data_2d_file_name)\n",
    "with open(data_2d_file_name, 'rb') as file:\n",
    "    data_2d = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false,
    "tags": [
     "AST"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using previously downloaded file show.py\n"
     ]
    }
   ],
   "source": [
    "show_file = 'show.py'\n",
    "retrieve(show_file)\n",
    "from show import show_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.1 (Exam Style except for Running the Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(SVC(kernel='rbf'), ad_data, 'RBF SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.2 (Exam Style except for Running the Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm, _ = experiment(SVC(kernel='linear'), data_2d, 'linear SVM')\n",
    "show_classification(linear_svm, data_2d['train'], 'training', 'linear SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_classification(lienar_svm, data_2d['test'], 'test', 'linear SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.3 (Exam Style except for Running the Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_svm = experiment(SVC(kernel='rbf'), data_2d, 'RBF SVM')\n",
    "show_classification(rbf_svm, data_2d['train'], 'training', 'RBF SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_classification(rbf_svm, data_2d['train'], 'test', 'RBF SVM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
