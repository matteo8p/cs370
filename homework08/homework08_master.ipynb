{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "<style>\n",
    "    @media print{\n",
    "        body {\n",
    "            position:relative !important;\n",
    "        }\n",
    "        .celltag_new_page {\n",
    "            page-break-before: always !important;\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "# COMPSCI 371 Homework 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partners: Brian Janger, Matthew Wang, Caleb Watson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AT"
    ]
   },
   "source": [
    "### Problem 0 (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 1: Kernels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.1 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step, we put the conditions in matrix form: \n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "K(x_1,x_1) & K(x_1,x_2)\\\\\n",
    "K(x_2,x_1) & K(x_2,x_2)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & 2\\\\\n",
    "2 & 1\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that it is a valid Kernel, we test if the matrix is positive semi-definite. We find the eigenvalues of the symmetrical matrix. \n",
    "\n",
    "The eigenvalues of the matrix are $\\lambda_1=-1, \\lambda_2=3$. Since one of the eigenvalues is negative, the matrix is not positive semi-definite. Therefore, no function $\\rho(x)$ exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.2 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function $K(x,\\xi)=(x^T\\xi+c)^2$ in $R^3$ is $K(x,\\xi)=(x^T\\xi+c)^2=(x_1\\xi_1 + x_2\\xi_2 + x_3\\xi_3 + c)^2$\n",
    "\n",
    "Expanding the function out, we get \n",
    "\n",
    "$x_1^2\\xi_1^2 + x_2^2\\xi_2^2 + x_3^2\\xi_3^2 + 2x_1x_2\\xi_1\\xi_2 + 2x_1x_3\\xi_1\\xi_3 + 2x_2x_3\\xi_2\\xi_3 + 2x_1\\xi_1c + 2x_2\\xi_2c + 2x_3\\xi_3c + c^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore \n",
    "\n",
    "$\\rho(x)=\\rho(x_1,x_2,x_3)=(c,x_1,x_2,x_3,\\sqrt{2}x_1x_2,\\sqrt{2}x_1x_3,\\sqrt{2}x_2x_3,\\sqrt{2}x_1,\\sqrt{2}x_2,\\sqrt{2}x_3)^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 monomials, so $e=10$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.3 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$e={d+k \\choose k}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false,
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n, d = 20, 30\n",
    "x = np.random.randn(n, d)\n",
    "\n",
    "#def check_rbf_kernel(x, sigma=1.):\n",
    "    \n",
    "    \n",
    "#check_rbd_kernel(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 2: The Representer Theorem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os import path as osp\n",
    "\n",
    "def retrieve(file_name, semester='fall22', course='371', homework=8):\n",
    "    if osp.exists(file_name):\n",
    "        print('Using previously downloaded file {}'.format(file_name))\n",
    "    else:\n",
    "        fmt = 'https://www2.cs.duke.edu/courses/{}/compsci{}/homework/{}/{}'\n",
    "        url = fmt.format(semester, course, homework, file_name)\n",
    "        urlretrieve(url, file_name)\n",
    "        print('Downloaded file {}'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false,
    "tags": [
     "AST"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using previously downloaded file ad.pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "ad_data_file = 'ad.pickle'\n",
    "retrieve(ad_data_file, homework=6)\n",
    "with open(ad_data_file, 'rb') as file:\n",
    "    ad_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.1 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The representer theorem does hold for $L_{\\text{reg}}(v) = \\left\\lVert v \\right\\rVert^2 + CL_T(v)$ because it matches the general formulation of the representer theorem (assuming $v$ is a training observation where $v \\in \\mathbb{R}^N$). The representer theorem requires a strictly increasing function $R$ from $\\mathbb{R}_+$ to $\\mathbb{R}$ and any function $S$ from $\\mathbb{R}^N$ to $\\mathbb{R}$.\n",
    "\n",
    "We see that for the function $R(a) = a^2$ and the function $S(v) = CL_T(v)$, the LRC risk function can be rewritten as $L_{\\text{reg}}(v) = R(\\left\\lVert v \\right\\rVert) + S(v)$, which matches the general formulation of the representer theorem. Therefore, the reprenter theorem holds for LRCs trained with regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.2 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The representer theorem does not hold for $L_T(v) = -\\frac{1}{N}\\sum\\limits_{n=1}^N[y_n\\log p_n + (1-y_n)\\log(1-p_n)]$ where $p_n = \\frac{1}{e^{-w^Tx_n-b}}$ because we violate the proof assumption that we have a strictly increasing function $R$ from $\\mathbb{R}_+$ to $\\mathbb{R}$ in our training risk function. All we have in the standard cross-entropy loss function is a single term, which serves as the function $S$ from $\\mathbb{R}^N$ to $R$.\n",
    "\n",
    "The proof is violated specifically at inequality (6) in the class notes, where we use the function $R$ to justify the implication that $R(\\left\\lVert w \\right\\rVert) < R(\\left\\lVert w* \\right\\rVert)$ (where $w = w^* - u, u \\neq 0$). \n",
    "\n",
    "Since the proof also shows that our function $S$ has the property $S(w^Tx_1+b,...,w^Tx_N+b) = S((w^*)^Tx_1+b,...,(w^*)^Tx_N+b)$, the proof relies on the fact that inequality (6) exists to prove that $L(w,b) < L(w^*,b)$. Since this statement isn't valid anymore, we can't say that $w^*$ is the optimal vector, and therefore the vector $u \\in X^\\perp$ can be nonzero, meaning the hyerplane cannot be expressed as a linear combination of the input vectors $x_1,...,x_N \\in X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false,
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate(h, data, h_name):\n",
    "    def accuracy(s):\n",
    "        sx, sy = s['x'], s['y']\n",
    "        return h.score(sx, sy) * 100\n",
    "\n",
    "    train, test = data['train'], data['test']\n",
    "    f = '{:s}:\\n\\ttraining accuracy is {:.2f} percent,' +\\\n",
    "        '\\n\\ttest accuracy is {:.2f} percent,'\n",
    "    print(f.format(h_name, accuracy(train), accuracy(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRC:\n",
      "\ttraining accuracy is 99.15 percent,\n",
      "\ttest accuracy is 96.78 percent,\n",
      "\tthe best value for the regularization constant C is 0.0610540229658533,\n",
      "\tthere are 1179 training samples of dimensionality 1430,\n",
      "\tthe number of linearly independent training samples is 561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kl/s_1z5b7d0hd63whkbcpmn10w0000gn/T/ipykernel_72569/2949515679.py:33: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  beta = np.linalg.lstsq(a=np.transpose(data), b=vector)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1685970\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1179 is out of bounds for axis 0 with size 1179",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         np\u001b[38;5;241m.\u001b[39mappend(sum_array, beta[d]\u001b[38;5;241m*\u001b[39mdata[d])\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(vector \u001b[38;5;241m-\u001b[39m sum_array)\n\u001b[0;32m---> 43\u001b[0m \u001b[43mexperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mad_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLRC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36mexperiment\u001b[0;34m(h, data, h_name)\u001b[0m\n\u001b[1;32m     21\u001b[0m w \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     22\u001b[0m random_w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;28mlen\u001b[39m(w))\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mthe residual for the optimal vector found by the training algorithm is \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mresidual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mthe residual for a random vector is \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(residual(random_w, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m])))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h, w\n",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36mresidual\u001b[0;34m(vector, data)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39msize(data))\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(np\u001b[38;5;241m.\u001b[39msize(data)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 39\u001b[0m     np\u001b[38;5;241m.\u001b[39mappend(sum_array, \u001b[43mbeta\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m*\u001b[39mdata[d])\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(vector \u001b[38;5;241m-\u001b[39m sum_array)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1179 is out of bounds for axis 0 with size 1179"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def experiment(h, data, h_name): \n",
    "    # cross-validates classifier and fits to determine a regularization constant\n",
    "    cs = np.logspace(-3,2,15)\n",
    "    h = GridSearchCV(h, {'C': cs})\n",
    "    h = h.fit(data['train']['x'], data['train']['y'])\n",
    "    h = h.best_estimator_\n",
    "    \n",
    "    # obtain training and testing accuracy\n",
    "    evaluate(h, data, h_name)\n",
    "\n",
    "    #printing remaining required information\n",
    "    print('\\tthe best value for the regularization constant C is {},'.format(h.C))\n",
    "    print('\\tthere are {} training samples of dimensionality {},'.format(len(data['train']['x']), len(data['train']['x'][0])))\n",
    "    print('\\tthe number of linearly independent training samples is {}'.format(np.linalg.matrix_rank(data['train']['x'])))\n",
    "    \n",
    "    # calculating residuals for optimal and random vectors (only applicable to linear classifiers)\n",
    "    if hasattr(h, 'coef_'):\n",
    "        w = h.coef_.flatten()\n",
    "        random_w = np.random.rand(len(w))\n",
    "        \n",
    "        print('\\tthe residual for the optimal vector found by the training algorithm is %.'.format(residual(w, data['train']['x'])))\n",
    "        print('\\tthe residual for a random vector is %.'.format(residual(random_w, data['train']['x'])))\n",
    "        return h, w\n",
    "    else:\n",
    "        return h\n",
    "    \n",
    "# helper function to calculate residual\n",
    "def residual(vector, data):\n",
    "    # computing coefficients from the data points\n",
    "    beta = np.linalg.lstsq(a=np.transpose(data), b=vector)[0]\n",
    "\n",
    "    sum_array = np.empty(shape=(np.size(data[0]),1))\n",
    "    \n",
    "    print(np.size(data))\n",
    "    for d in range(np.size(data)-1):\n",
    "        np.append(sum_array, beta[d]*data[d])\n",
    "        \n",
    "    return np.linalg.norm(vector - sum_array)\n",
    "    \n",
    "experiment(LogisticRegression(max_iter=1000), ad_data, 'LRC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such high dimesionality - low probability of picking a vector at rnadom and having a residual close to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm, w_svm = experiment(SVC(kernel='linear'), ad_data, 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 3: Linear and Nonlinear SVMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false,
    "tags": [
     "AST"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using previously downloaded file data.pickle\n"
     ]
    }
   ],
   "source": [
    "data_2d_file_name = 'data.pickle'\n",
    "retrieve(data_2d_file_name)\n",
    "with open(data_2d_file_name, 'rb') as file:\n",
    "    data_2d = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false,
    "tags": [
     "AST"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using previously downloaded file show.py\n"
     ]
    }
   ],
   "source": [
    "show_file = 'show.py'\n",
    "retrieve(show_file)\n",
    "from show import show_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.1 (Exam Style except for Running the Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment(SVC(kernel='rbf'), ad_data, 'RBF SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.2 (Exam Style except for Running the Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svm, _ = experiment(SVC(kernel='linear'), data_2d, 'linear SVM')\n",
    "show_classification(linear_svm, data_2d['train'], 'training', 'linear SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_classification(lienar_svm, data_2d['test'], 'test', 'linear SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.3 (Exam Style except for Running the Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_svm = experiment(SVC(kernel='rbf'), data_2d, 'RBF SVM')\n",
    "show_classification(rbf_svm, data_2d['train'], 'training', 'RBF SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_classification(rbf_svm, data_2d['train'], 'test', 'RBF SVM')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
