{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "<style>\n",
    "    @media print{\n",
    "        body {\n",
    "            position:relative !important;\n",
    "        }\n",
    "        .celltag_new_page {\n",
    "            page-break-before: always !important;\n",
    "        }\n",
    "    }\n",
    "</style>\n",
    "# COMPSCI 371 Homework 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Members: Brian Janger, Matthew Wang, Caleb Watson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AT"
    ]
   },
   "source": [
    "### Problem 0 (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 1: Mathematics of Correlation and Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.1 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider term $j$ in the convolution $a *_- b$, which we call $(a *_- b)_j$:\n",
    "\n",
    "$(a*_-b)_j = \\sum\\limits_i a_ib_{j-i}$\n",
    "\n",
    "Now, let $k = j - i$ and consider term $j$ in the convolution $b *_- a$, which we call $(b *_- a)_j$:\n",
    "\n",
    "$(b*_-a)_j = \\sum\\limits_i b_ia_{j-i} = \\sum\\limits_k a_kb_{j-k} = (a*_-b)_j$\n",
    "\n",
    "We can see through the change of variables that the two terms of the sums are the same, so convolution is commutative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 1.2 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider term  𝑗\n",
    "j\n",
    "  in the correlation  𝑎∗+𝑏\n",
    "a\n",
    "∗\n",
    "+\n",
    "b\n",
    "  which we call  (𝑎∗+𝑏)𝑗\n",
    "(\n",
    "a\n",
    "∗\n",
    "+\n",
    "b\n",
    ")\n",
    "j\n",
    " :\n",
    "(𝑎∗+𝑏)𝑗=∑𝑖𝑎𝑖𝑏𝑗+𝑖\n",
    "(\n",
    "a\n",
    "∗\n",
    "+\n",
    "b\n",
    ")\n",
    "j\n",
    "=\n",
    "∑\n",
    "i\n",
    "a\n",
    "i\n",
    "b\n",
    "j\n",
    "+\n",
    "i\n",
    " \n",
    "Let  𝑘=𝑗+𝑖\n",
    "k\n",
    "=\n",
    "j\n",
    "+\n",
    "i\n",
    "  and consider term  𝑗\n",
    "j\n",
    "  in the correlation  𝑏∗+𝑎\n",
    "b\n",
    "∗\n",
    "+\n",
    "a\n",
    " , which we call  (𝑏∗+𝑎)𝑗\n",
    "(\n",
    "b\n",
    "∗\n",
    "+\n",
    "a\n",
    ")\n",
    "j\n",
    " :\n",
    "(𝑏∗+𝑎)𝑗=∑𝑖𝑏𝑖𝑎𝑗+𝑖=∑𝑘𝑎𝑘𝑏𝑘−𝑗=∑𝑘𝑎𝑘𝑏(−𝑗)+𝑘=(𝑎∗+𝑏)−𝑗\n",
    "(\n",
    "b\n",
    "∗\n",
    "+\n",
    "a\n",
    ")\n",
    "j\n",
    "=\n",
    "∑\n",
    "i\n",
    "b\n",
    "i\n",
    "a\n",
    "j\n",
    "+\n",
    "i\n",
    "=\n",
    "∑\n",
    "k\n",
    "a\n",
    "k\n",
    "b\n",
    "k\n",
    "−\n",
    "j\n",
    "=\n",
    "∑\n",
    "k\n",
    "a\n",
    "k\n",
    "b\n",
    "(\n",
    "−\n",
    "j\n",
    ")\n",
    "+\n",
    "k\n",
    "=\n",
    "(\n",
    "a\n",
    "∗\n",
    "+\n",
    "b\n",
    ")\n",
    "−\n",
    "j\n",
    " \n",
    "We can see that term  𝑗\n",
    "j\n",
    "  in the correlation  𝑏∗+𝑎\n",
    "b\n",
    "∗\n",
    "+\n",
    "a\n",
    "  is the same as term  (−𝑗)\n",
    "(\n",
    "−\n",
    "j\n",
    ")\n",
    "  in the correlation  𝑎∗+𝑏\n",
    "a\n",
    "∗\n",
    "+\n",
    "b\n",
    " , so the sums are in reverse order of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 2: Coding Correlation and Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os import path as osp\n",
    "\n",
    "\n",
    "def retrieve(file_name, semester='fall22', course='371', homework=10):\n",
    "    if osp.exists(file_name):\n",
    "        print('Using previously downloaded file {}'.format(file_name))\n",
    "    else:\n",
    "        fmt = 'https://www2.cs.duke.edu/courses/{}/compsci{}/homework/{}/{}'\n",
    "        url = fmt.format(semester, course, homework, file_name)\n",
    "        urlretrieve(url, file_name)\n",
    "        print('Downloaded file {}'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file oracle.py\n"
     ]
    }
   ],
   "source": [
    "retrieve('oracle.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "from oracle import oracle, show\n",
    "\n",
    "\n",
    "def test(function, a=[1, 2, 3, 4], b=[5, 6]):\n",
    "    for operator in ('correlate', 'convolve'):\n",
    "        for mode in ('full', 'same', 'valid'):\n",
    "            for f, g in ((a, b), (b, a)):\n",
    "                c = function(operator, f, g, mode)\n",
    "                show(operator, f, g, mode, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 3: Back-Propagation through a Convolution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 3.1 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AT"
    ]
   },
   "source": [
    "$$\n",
    "\\begin{eqnarray*}\n",
    "c_0 &=& a_0 b_0 \\\\\n",
    "c_1 &=& a_0 b_1 + a_1 b_0 \\\\\n",
    "c_2 &=& a_0 b_2 + a_1 b_1 + a_2 b_0 \\\\\n",
    "c_3 &=& a_1 b_2 + a_2 b_1 + a_3 b_0 \\\\\n",
    "c_4 &=& a_2 b_2 + a_3 b_1 \\\\\n",
    "c_5 &=& a_3 b_2\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 4: Network Back-Propagation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tiny network](https://courses.cs.duke.edu//fall22/compsci371/homework/10/tiny_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 4.1 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a_1 = -4$, $z_1 = 0$, $a_2 = 7$, $z_2 = 7$, $\\hat{y} = 3$\n",
    "\n",
    "quadratic loss $= \\frac{1}{2}[h(x)-y]^2 = \\frac{1}{2}[3-5]^2 = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 4.2 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "T"
    ]
   },
   "source": [
    "$$\n",
    "\\begin{eqnarray*}\n",
    "g_{\\hat{y}} &=& \\hat{y} - y = 3 - 5 = -2 \\\\\n",
    "g_b &=& g_{\\hat{y}} \\cdot 1 = -2 \\\\\n",
    "g_{v_2} &=& g_{\\hat{y}}\\ z_2 = -2 \\cdot 7 = -14\\\\\n",
    "g_{v_1} &=& g_{\\hat{y}}\\ z_1 = -2 \\cdot 0 = 0\\\\\n",
    "g_{z_2} &=& g_{\\hat{y}}\\ v_2 = -2 \\cdot 1 = -2\\\\\n",
    "g_{z_1} &=& g_{\\hat{y}}\\ v_1 = -2 \\cdot 4 = -8\\\\\n",
    "g_{a_2} &=& g_{\\hat{y}}\\ v_2\\ \\sigma(a_2) = -2 \\cdot 1 \\cdot 1 = -2 \\\\\n",
    "g_{a_1} &=& g_{\\hat{y}}\\ v_1\\ \\sigma(a_1) = -2 \\cdot 4 \\cdot 0 = 0 \\\\\n",
    "g_{u_{23}} &=& g_{a_2}\\ x_3 = -2 \\cdot 2 = -4 \\\\\n",
    "g_{u_{22}} &=& g_{a_2}\\ x_2 = -2 \\cdot -2 = 4 \\\\\n",
    "g_{u_{21}} &=& g_{a_2}\\ x_1 = -2 \\cdot -3 = 6 \\\\\n",
    "g_{u_{13}} &=& g_{a_1}\\ x_3 = 0 \\cdot 2 = 0 \\\\\n",
    "g_{u_{12}} &=& g_{a_1}\\ x_2 = 0 \\cdot -2 = 0 \\\\\n",
    "g_{u_{11}} &=& g_{a_1}\\ x_1 = 0 \\cdot -3 = 0\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "T"
    ]
   },
   "source": [
    "Collecting the relevant entries in reverse order yields\n",
    "\n",
    "$$\n",
    "g_{\\mathbf{w}} = (0, 0, 0, 6, 4, -4, 0, -14, -2)\\;.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "## Part 5: MNIST Digit Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file mnist.pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "file_name = 'mnist.pickle'\n",
    "retrieve(file_name)\n",
    "with open(file_name, 'rb') as file:\n",
    "    mnist = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "def print_result(h, d):\n",
    "    accuracy = {\n",
    "        'train': h.score(d['train']['x'], d['train']['y']) * 100,\n",
    "        'test': h.score(d['test']['x'], d['test']['y']) * 100\n",
    "    }\n",
    "    print('training accuracy: {:.2f} percent'.format(accuracy['train']))\n",
    "    print('test accuracy: {:.2f} percent'.format(accuracy['test']))\n",
    "    max_points = 20\n",
    "    p = (accuracy['test'] - 90.) / (96. - 90.) * max_points\n",
    "    p = min((max_points, max((0, p))))\n",
    "    p = round(p)\n",
    "    print('{} out of {} points'.format(p, max_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "\n",
    "def experiment(data, hidden_layer_sizes, max_iter, alpha,\n",
    "               learning_rate_init, verbose=False):\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        max_iter=max_iter,\n",
    "        alpha=alpha,\n",
    "        learning_rate_init=learning_rate_init,\n",
    "        learning_rate='constant',\n",
    "        solver='sgd',\n",
    "        random_state=1,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\n",
    "            \"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n",
    "        mlp.fit(data['train']['x'], data['train']['y'])\n",
    "    print_result(mlp, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 5.1 (Exam Style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a blind classification were run, with a balanced MNIST dataset, the expected accuracy of the blind classifier would be 1 divided by the total number of classification groups, which in this case would be 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "AST"
    ]
   },
   "source": [
    "### Problem 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": [
     "AT"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68690960\n",
      "Iteration 2, loss = 0.21285632\n",
      "Iteration 3, loss = 0.13624568\n",
      "Iteration 4, loss = 0.09436123\n",
      "Iteration 5, loss = 0.06508058\n",
      "Iteration 6, loss = 0.04197691\n",
      "Iteration 7, loss = 0.02729570\n",
      "Iteration 8, loss = 0.02040278\n",
      "Iteration 9, loss = 0.01347171\n",
      "Iteration 10, loss = 0.01071940\n",
      "Iteration 11, loss = 0.00767475\n",
      "Iteration 12, loss = 0.00471607\n",
      "Iteration 13, loss = 0.00173934\n",
      "Iteration 14, loss = 0.00113444\n",
      "Iteration 15, loss = 0.00089434\n",
      "Iteration 16, loss = 0.00066942\n",
      "Iteration 17, loss = 0.00057737\n",
      "Iteration 18, loss = 0.00051437\n",
      "Iteration 19, loss = 0.00047284\n",
      "Iteration 20, loss = 0.00043520\n",
      "Iteration 21, loss = 0.00040647\n",
      "Iteration 22, loss = 0.00038114\n",
      "training accuracy: 100.00 percent\n",
      "test accuracy: 96.19 percent\n",
      "20 out of 20 points\n"
     ]
    }
   ],
   "source": [
    "experiment(\n",
    "    mnist,\n",
    "    hidden_layer_sizes=(100, 25),\n",
    "    max_iter=22,\n",
    "    alpha=1e-8,\n",
    "    learning_rate_init=0.15,\n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
